{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5391745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_version = tf.__version__\n",
    "keras_version = tf.keras.__version__\n",
    "python_version = platform.python_version()\n",
    "\n",
    "print(f\"TensorFlow version installed: {tf_version}\")\n",
    "print(f\"Keras version installed: {keras_version}\")\n",
    "\n",
    "if tf_version != \"2.10.1\":\n",
    "    raise ValueError(f\"Incorrect TensorFlow version: Required 2.10.1, but found {tf_version}.\")\n",
    "if keras_version != \"2.10.0\":\n",
    "    raise ValueError(f\"Incorrect Keras version: Required version 2.10.0, but found {keras_version}.\")\n",
    "if python_version != \"3.10.11\":\n",
    "    raise ValueError(f\"Incorrect Python version: Required version 3.10.11, but found {python_version}.\")\n",
    "\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import datetime\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import random\n",
    "import pennylane.numpy as qnp\n",
    "import warnings\n",
    "import pennylane as qml\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "def set_random_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    qnp.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8593427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df = pd.read_csv(\"./abalone.csv\")\n",
    "\n",
    "# Define numeric columns for outlier detection\n",
    "numeric_cols = ['Diameter', 'Length', 'Height',\n",
    "                'Whole weight', 'Shucked weight',\n",
    "                'Viscera weight', 'Shell weight']\n",
    "\n",
    "# Remove outliers using IQR\n",
    "Q1 = df[numeric_cols].quantile(0.25)\n",
    "Q3 = df[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Keep only rows without extreme outliers\n",
    "mask = ~((df[numeric_cols] < (Q1 - 2 * IQR)) | (df[numeric_cols] > (Q3 + 2 * IQR))).any(axis=1)\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "# Prepare target and features\n",
    "y_train = df['Rings'].values.reshape(-1, 1)\n",
    "X_train = df.drop(columns=['Rings'])\n",
    "X_train = pd.get_dummies(X_train, columns=[\"Sex\"], drop_first=True, dtype=int)\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train[numeric_cols] = scaler_X.fit_transform(X_train[numeric_cols])\n",
    "y_train = scaler_y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumKerasLayerWithDropout(qml.qnn.KerasLayer):\n",
    "    def __init__(self, qnode, weight_shapes, output_dim, activation=None, **kwargs):\n",
    "        self.activation_fn = tf.keras.activations.get(activation)\n",
    "        super().__init__(qnode=qnode, weight_shapes=weight_shapes, output_dim=output_dim, **kwargs)\n",
    "        \n",
    "    def _signature_validation(self, qnode, weight_shapes):\n",
    "        sig = inspect.signature(qnode.func).parameters\n",
    "        param_kinds = [p.kind for p in sig.values()]\n",
    "        \n",
    "        if self.input_arg not in sig:\n",
    "            raise ValueError(f\"QNode must have an input parameter named {self.input_arg}\")\n",
    "        \n",
    "        expected_weight_params = set(sig.keys()) - {self.input_arg, 'training_flag'}\n",
    "        provided_weight_keys = set(weight_shapes.keys())\n",
    "        \n",
    "        if expected_weight_params != provided_weight_keys:\n",
    "            missing = expected_weight_params - provided_weight_keys\n",
    "            extra = provided_weight_keys - expected_weight_params\n",
    "            err_msg = f\"Mismatch between QNode parameters and weight_shapes keys.\\n\"\n",
    "            err_msg += f\" - QNode signature parameters (excluding input '{self.input_arg}' and 'training_flag'): {expected_weight_params}\\n\"\n",
    "            err_msg += f\" - Weight shape keys provided: {provided_weight_keys}\\n\"\n",
    "            if missing: err_msg += f\" - Missing weight shapes for: {missing}\\n\"\n",
    "            if extra: err_msg += f\" - Extra weight shapes provided for: {extra}\\n\"\n",
    "            raise ValueError(f\"Must specify a shape for every non-input parameter (excluding 'training_flag') in the QNode. {err_msg}\")\n",
    "        \n",
    "        if inspect.Parameter.VAR_POSITIONAL in param_kinds:\n",
    "            raise ValueError(\"QNode cannot have variable positional arguments\")\n",
    "   \n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            is_training_bool = tf.constant(False, dtype=tf.bool)\n",
    "        else:\n",
    "            is_training_bool = tf.cast(training, dtype=tf.bool)\n",
    "\n",
    "        qnode_kwargs = {\n",
    "            self.input_arg: inputs,\n",
    "            **self.qnode_weights\n",
    "        }\n",
    "\n",
    "        if 'training_flag' in inspect.signature(self.qnode.func).parameters:\n",
    "            qnode_kwargs['training_flag'] = is_training_bool\n",
    "            \n",
    "        output = self.qnode(**qnode_kwargs)\n",
    "        \n",
    "        if not isinstance(output, list) or not output:\n",
    "            raise ValueError(\"QNode must return a list of measurements.\")\n",
    "        if len(output) > 1:\n",
    "            output = tf.stack(output, axis=1)\n",
    "        else:\n",
    "            output = tf.expand_dims(output[0], axis=-1)\n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'activation': tf.keras.activations.serialize(self.activation_fn)})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66eaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qnode(n_qubits, ansatz_type, embedding_type, quantum_dropout_rate, dropoutScaling):\n",
    "    device_type = \"default.qubit\"\n",
    "    if(device_type != \"default.qubit\"):\n",
    "        warnings.warn(f\"Device type '{device_type}' is not the expected 'default.qubit'. Potential compatibility issues.\", UserWarning)\n",
    "        \n",
    "    dev = qml.device(device_type, wires=n_qubits, seed=seed_value)\n",
    "\n",
    "    @qml.qnode(dev, interface='tf')\n",
    "    def qnode(inputs, weights, training_flag):\n",
    "        #tf.print(\"\\ntraining_flag:\", training_flag, summarize=-1)\n",
    "\n",
    "        # Embedding\n",
    "        if embedding_type == \"AmplitudeEmbedding\":\n",
    "            qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)\n",
    "        elif embedding_type == \"AngleEmbedding\":\n",
    "            qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "\n",
    "        n_layers = tf.shape(weights)[0]\n",
    "\n",
    "        # Generate random values for dropout mask\n",
    "        rand_vals = tf.random.uniform(tf.shape(weights), 0.0, 1.0, dtype=tf.float32)\n",
    "\n",
    "        # Determine the mask based on the training_flag argument\n",
    "        keep_mask = tf.cond(training_flag, lambda: rand_vals >= quantum_dropout_rate, lambda: tf.ones_like(rand_vals, dtype=tf.bool))\n",
    "        #tf.print(f\"\\nkeep_mask:{keep_mask}\")\n",
    "\n",
    "        for layer_i in range(n_layers):\n",
    "            if ansatz_type == 'BasicEntangler':\n",
    "                for qubit_i in range(n_qubits):\n",
    "                    angle = weights[layer_i, qubit_i]\n",
    "                    current_mask_value = keep_mask[layer_i, qubit_i]\n",
    "                    \n",
    "                    if quantum_dropout_rate == 0.0:\n",
    "                        angle_eff = angle\n",
    "                    else:\n",
    "                        angle_eff = tf.where(current_mask_value,angle,tf.constant(0.0, dtype=weights.dtype))\n",
    "                        \n",
    "                        angle_eff = tf.cond(\n",
    "                            tf.logical_and(training_flag, dropoutScaling),\n",
    "                            lambda: angle_eff / (1.0 - quantum_dropout_rate),\n",
    "                            lambda: angle_eff\n",
    "                        )\n",
    "                    qml.RY(angle_eff, wires=qubit_i)\n",
    "                for qubit_i in range(n_qubits - 1):\n",
    "                    qml.CNOT(wires=[qubit_i, qubit_i + 1])\n",
    "\n",
    "            elif ansatz_type == 'StronglyEntangling':\n",
    "                current_weights_layer = weights[layer_i]\n",
    "                current_mask_layer = keep_mask[layer_i]\n",
    "                for qubit_i in range(n_qubits):\n",
    "                    params = current_weights_layer[qubit_i]\n",
    "                    mask_params = current_mask_layer[qubit_i]\n",
    "                    \n",
    "                    if quantum_dropout_rate == 0.0:\n",
    "                        masked_params = params\n",
    "                    else:\n",
    "                        masked_params = tf.where(mask_params,params,tf.zeros_like(params, dtype=weights.dtype))\n",
    "                        \n",
    "                        masked_params = tf.cond(\n",
    "                            tf.logical_and(training_flag, dropoutScaling),\n",
    "                            lambda: masked_params / (1.0 - quantum_dropout_rate),\n",
    "                            lambda: masked_params\n",
    "                        )\n",
    "                    #tf.print(f\"params: {masked_params}\")\n",
    "                    qml.Rot(masked_params[0], masked_params[1], masked_params[2], wires=qubit_i)\n",
    "\n",
    "                current_range = (layer_i % n_qubits) + 1\n",
    "\n",
    "                for qubit_i in range(n_qubits):\n",
    "                    target_qubit = (qubit_i + current_range) % n_qubits\n",
    "                    if qubit_i != target_qubit:\n",
    "                        qml.CNOT(wires=[qubit_i, target_qubit])\n",
    "                    \n",
    "        measurements = []\n",
    "\n",
    "        measurements.extend([qml.expval(qml.PauliZ(i)) for i in range(n_qubits)])\n",
    "\n",
    "        if n_qubits > 1:\n",
    "            def next_qubit(i, n):\n",
    "                return (i + 1) % n\n",
    "            \n",
    "            for i in range(n_qubits):\n",
    "                j = next_qubit(i, n_qubits)\n",
    "                measurements.append(qml.expval(qml.PauliZ(i) @ qml.PauliZ(j)))\n",
    "        else:\n",
    "            measurements.append(qml.expval(qml.PauliX(0)))\n",
    "\n",
    "        return measurements\n",
    "\n",
    "    return qnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad936a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(weight_shapes, n_qubits, ansatz_type, num_classical_layers, num_classical_neurons, quantum_dropout_rate, embedding_type, input_dim, dropout_scaling):\n",
    "    \n",
    "    if input_dim < 1:\n",
    "        raise ValueError(\"input_dim must be bigger than 0\")\n",
    "    if not 0.0 <= quantum_dropout_rate < 1.0:\n",
    "        raise ValueError(\"quantum_dropout_rate must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "    if not (1 <= n_qubits < 16):\n",
    "        raise ValueError(\"n_qubits must be between 1 and 15\")\n",
    "    if embedding_type not in [\"AmplitudeEmbedding\", \"AngleEmbedding\"]:\n",
    "        raise ValueError(\"Please use a valid embedding_type ('AmplitudeEmbedding' or 'AngleEmbedding')\")\n",
    "    if n_qubits >= 9 and embedding_type == \"AmplitudeEmbedding\":\n",
    "        raise ValueError(f\"AmplitudeEmbedding requires 2^{n_qubits} features. For n_qubits={n_qubits}, this is {2**n_qubits}, which might be too large.\")\n",
    "    if ansatz_type not in [\"BasicEntangler\", \"StronglyEntangling\"]:\n",
    "        raise ValueError(\"Please use a valid ansatz_type ('BasicEntangler' or 'StronglyEntangling')\")\n",
    "    if \"weights\" not in weight_shapes:\n",
    "        raise ValueError(f\"Missing 'weights' in weight_shapes for {ansatz_type}\")\n",
    "    if len(num_classical_neurons) != num_classical_layers:\n",
    "        raise ValueError(\"Length of num_classical_neurons must match num_classical_layers\")\n",
    "    if (ansatz_type == 'StronglyEntangling' and len(weight_shapes[\"weights\"]) != 3) or (ansatz_type == \"BasicEntangler\" and len(weight_shapes[\"weights\"]) != 2):\n",
    "        raise ValueError(f\"Invalid weight_shapes for {ansatz_type}\")\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    set_random_seed(seed_value)\n",
    "    \n",
    "    qnode_instance = create_qnode(n_qubits, ansatz_type, embedding_type, quantum_dropout_rate, dropout_scaling)\n",
    "    quantum_layer = QuantumKerasLayerWithDropout(qnode_instance, weight_shapes, output_dim=2*n_qubits) #2*n_qubits because I am doing 2 measurements per qubit\n",
    "    \n",
    "    model = tf.keras.Sequential(name=f\"QuantumModel_{ansatz_type}_{embedding_type}_dropout{quantum_dropout_rate}_qubits{n_qubits}_depth{weight_shapes['weights'][0]}\")\n",
    "    \n",
    "    model.add(tf.keras.layers.Input(shape=(input_dim,), name=\"Input\"))\n",
    "    \n",
    "    if embedding_type == \"AmplitudeEmbedding\":\n",
    "        dense_units = 2**n_qubits\n",
    "        model.add(tf.keras.layers.Dense(dense_units, name=\"Embedding_Dense\"))\n",
    "    else: \n",
    "        dense_units = n_qubits\n",
    "        model.add(tf.keras.layers.Dense(dense_units, name=\"Embedding_Dense\"))\n",
    "        \n",
    "    model.add(quantum_layer)\n",
    "\n",
    "    for i in range(num_classical_layers):\n",
    "        units = num_classical_neurons[i]\n",
    "        model.add(tf.keras.layers.Dense(units, kernel_initializer=tf.keras.initializers.HeNormal(), name=f\"Classical_Dense_{i+1}\"))\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, name=\"Output_Dense\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            line_count = sum(1 for _ in f)\n",
    "        return line_count\n",
    "    except FileNotFoundError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a254c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 8\n",
    "learning_rate = 0.0015\n",
    "num_classical_neurons = [8]\n",
    "num_classical_layers = len(num_classical_neurons)\n",
    "\n",
    "filename = \"dataAbalone.txt\"\n",
    "\n",
    "model_num = 0\n",
    "num_models_tested = count_lines(filename)\n",
    "\n",
    "print(f\"Number of models tested: {num_models_tested}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ed881",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_r2_scores = []\n",
    "train_r2_scores = []\n",
    "param_count = 0\n",
    "\n",
    "for qubits in range(3,7):\n",
    "    for depth in range(3,7):\n",
    "        for dropout in [0.0, 0.1, 0.25]:\n",
    "            for ansatz in ['StronglyEntangling']:\n",
    "                for emb in [\"AngleEmbedding\", \"AmplitudeEmbedding\"]:\n",
    "                    for scaling in [True, False]:\n",
    "                        if(model_num < num_models_tested):\n",
    "                            model_num += 1\n",
    "                            continue\n",
    "\n",
    "                        if scaling == False and dropout == 0.0: #If dropout is 0.0, dropout scaling doesnt change the model\n",
    "                            print_info = f'{emb}, {ansatz}, Dropout Scaling: {scaling}, Qubits: {qubits}, Quantum Depth: {depth}, Dropout Rate: {dropout}, Train R2: {np.mean(train_r2_scores)}, Val R2: {np.mean(val_r2_scores)}, Quantum Params: {3*qubits*depth}, Total Params: {param_count}'\n",
    "                            with open(filename, \"a\") as file:\n",
    "                                file.write(f\"{print_info}\\n\")\n",
    "                            model_num += 1\n",
    "                            print(f\"{print_info}\")\n",
    "                            continue\n",
    "\n",
    "                        weight_shapes = {\n",
    "                            \"weights\": (depth, qubits) if ansatz==\"BasicEntangler\" else (depth, qubits, 3), #3 is the default number of parameters per qubit for StronglyEntangling\n",
    "                        }\n",
    "\n",
    "                        kf = KFold(n_splits=CV, shuffle=True, random_state=seed_value)\n",
    "\n",
    "                        train_r2_scores.clear()\n",
    "                        val_r2_scores.clear()\n",
    "                        \n",
    "                        print(f'\\n{emb}, {ansatz}, dropout scaling: {scaling}, qubits: {qubits}, quantum depth: {depth}, dropout rate: {dropout}\\n')\n",
    "                        param_count = 0\n",
    "                        \n",
    "                        for fold_number, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "                            X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                            y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "                            model = create_model(\n",
    "                                quantum_dropout_rate=dropout,\n",
    "                                n_qubits=qubits,\n",
    "                                weight_shapes=weight_shapes,\n",
    "                                ansatz_type=ansatz,\n",
    "                                num_classical_layers=num_classical_layers,\n",
    "                                num_classical_neurons=num_classical_neurons,\n",
    "                                embedding_type=emb,\n",
    "                                input_dim=len(X_train_cv.columns),\n",
    "                                dropout_scaling=scaling\n",
    "                            )\n",
    "\n",
    "                            if(fold_number == 0):\n",
    "                                model.summary()\n",
    "\n",
    "                            model.compile(\n",
    "                                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                loss=\"mse\",\n",
    "                                metrics=[tfa.metrics.RSquare(name='r2_score')]\n",
    "                            )\n",
    "\n",
    "                            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                patience=6,\n",
    "                                mode='min',\n",
    "                                restore_best_weights=True,\n",
    "                                verbose=0\n",
    "                            )\n",
    "                            \n",
    "                            with tf.device('/CPU:0'):\n",
    "                                model.fit(\n",
    "                                    X_train_cv, y_train_cv,\n",
    "                                    validation_data=(X_val_cv, y_val_cv),\n",
    "                                    epochs=12,\n",
    "                                    batch_size=48,\n",
    "                                    verbose=1,\n",
    "                                    callbacks=[early_stopping],\n",
    "                                    shuffle=True\n",
    "                                )\n",
    "                            \n",
    "                            param_count = model.count_params()\n",
    "                            \n",
    "                            y_train_pred = model.predict(X_train_cv)\n",
    "                            y_val_pred = model.predict(X_val_cv)\n",
    "                            \n",
    "                            train_r2 = r2_score(y_train_cv, y_train_pred)\n",
    "                            val_r2 = r2_score(y_val_cv, y_val_pred)\n",
    "\n",
    "                            train_r2_scores.append(train_r2)\n",
    "                            val_r2_scores.append(val_r2)\n",
    "\n",
    "                            print(f\"{datetime.datetime.now()} (fold: {fold_number + 1}/{CV}) Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}\")\n",
    "\n",
    "                        print_info = f'{emb}, {ansatz}, Dropout Scaling: {scaling}, Qubits: {qubits}, Quantum Depth: {depth}, Dropout Rate: {dropout}, Train R2: {np.mean(train_r2_scores)}, Val R2: {np.mean(val_r2_scores)}, Quantum Params: {3*qubits*depth}, Total Params: {param_count}'\n",
    "                        with open(filename, \"a\") as file:\n",
    "                            file.write(f\"{print_info}\\n\")\n",
    "                        model_num += 1\n",
    "                        print(f\"{print_info}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
